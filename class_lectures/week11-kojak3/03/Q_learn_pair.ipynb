{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today's pair will focus on [Q-Learning!](https://en.wikipedia.org/wiki/Q-learning)  \n",
    "\n",
    "\n",
    "**Background**\n",
    "Our problem space consists of an agent, a set of states, and a set \n",
    "of actions per state.  Our agent can move from state to state.\n",
    "\n",
    "For this problem, we have a few rooms in a building as such: \n",
    "\n",
    "<img src='q_learn.png'>\n",
    "\n",
    "We can represent the rooms as a graph:\n",
    "\n",
    "<img src='q_graph.png'>\n",
    "\n",
    "[example shamelessly extracted from here](http://mnemstudio.org/path-finding-q-learning-tutorial.htm)\n",
    "\n",
    "- The goal for this problem is to get outside of the building\n",
    "(target room is : #5).  The doors that lead to the goal have \n",
    "an instant reward of 100 and the doors that lead into other rooms \n",
    "have a value of 0 (as shown:) \n",
    "\n",
    "\n",
    "<img src='q_graph2.png'>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**ToDo**:\n",
    "\n",
    "1) Create the state matrix (aka R matrix) based on the diagram above\n",
    "where the rows represent the state, columns are actions, and table values are the reward values the correspond to the state and action.\n",
    "Note: if there is no door between rooms x & y, the reward value for moving from x to y is -1.  \n",
    "\n",
    "In the beginning, our agent has no knowledge of the environment and which \n",
    "rooms are more advantageous than others.  \n",
    "We want to create a Q-matrix which reflect the memory of what our agent has learned through experience of traveling from room to room. \n",
    "\n",
    "The agent will explore state to state until it reaches its goal; this \n",
    "process is called an episode.\n",
    "\n",
    "We'll use the following translation rule to update the Q matrix:    \n",
    "Q(state, action) = R(state, action) + Gamma * Max[Q(next state, all actions)]\n",
    "\n",
    "note the gamma parameter has a range of 0 to 1. \n",
    "(Think about what gamma is doing for us.  What happens when gamma is \n",
    "closer to 0 ?  What when its closer to 1? )\n",
    "\n",
    "2) Initialize Q matrix to zero.\n",
    "\n",
    "3) Update Q matrix: \n",
    "    - For each episode:\n",
    "\n",
    "    - Select a random initial state.\n",
    "\n",
    "    - Do While the goal state hasn't been reached.\n",
    "    - Select one among all possible actions for the current state.\n",
    "    - Compute: Q(state, action) = R(state, action) + Gamma * Max[Q(next state, all actions)]\n",
    "    - Set the next state as the current state.\n",
    "   \n",
    "   \n",
    "4) Now use the Q matrix!   \n",
    "\n",
    "    1. Set current state = initial state.\n",
    "    2. From current state, find the action with the highest Q value.\n",
    "    3. Set current state = next state.\n",
    "    4. Repeat Steps 2 and 3 until current state = goal state.\n",
    "\n",
    "How many steps does it take you to reach the goal now? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
