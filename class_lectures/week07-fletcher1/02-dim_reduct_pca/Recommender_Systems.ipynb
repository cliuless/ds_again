{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"You and I are essentially infinite choice-makers.  In every moment of \n",
    "our existence, we are in that field of possibilities where we have access to an infinity of choices\"\n",
    "- Deepak Chopra "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img_data/player1.jpeg'/>\n",
    "<img src='img_data/amazon.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommender systems** are a type of information filtering systems that seek to predict the rating that a user would give to an item. \n",
    "\n",
    "This process revolves around two main technologies: \n",
    "\n",
    "- ***Collaborative Filtering:*** Recommendations stem from users with similiar behavoir.  (Ex: Last.fm creates a 'station' of recommended songs by comparing user listening behavoir those with the listening behavoir of 'similiar' users.)\n",
    "    \n",
    "\n",
    "- ***Content-based Filtering:*** Examines the properties of the subject being recommended. (Ex: Pandora uses the properties of a song or artist in order to create a station )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1  Collaborative Filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.11  Memory Based Methods:   \n",
    "\n",
    "\n",
    "    * Neighbor-based Collaborative Filtering (CF) (Item-based / User-based algorithms )\n",
    "\n",
    "It is called 'memory based', because we need to store all the ratings in order to make recommedations\n",
    "#### 1.11 a)  User-User Collaborative Filtering  (Also known as K-NN collaborative filtering)\n",
    "\n",
    "    * Provide recommendations based off a similiar user's ratings\n",
    "\n",
    "First we start with the **Utility Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "% pylab inline\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "OUR BAND-RATING UTILITY MATRIX\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Angelica</th>\n",
       "      <th>Bill</th>\n",
       "      <th>Chan</th>\n",
       "      <th>Dan</th>\n",
       "      <th>Hailey</th>\n",
       "      <th>Jordyn</th>\n",
       "      <th>Sam</th>\n",
       "      <th>Veronica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Blues Traveler</th>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Broken Bells</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Deadmau5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Norah Jones</th>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phoenix</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Slightly Stoopid</th>\n",
       "      <td>1.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Strokes</th>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vampire Weekend</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Angelica  Bill  Chan  Dan  Hailey  Jordyn  Sam  Veronica\n",
       "Blues Traveler         3.5   2.0   5.0  3.0     NaN     NaN  5.0       3.0\n",
       "Broken Bells           2.0   3.5   1.0  4.0     4.0     4.5  2.0       NaN\n",
       "Deadmau5               NaN   4.0   1.0  4.5     1.0     4.0  NaN       NaN\n",
       "Norah Jones            4.5   NaN   3.0  NaN     4.0     5.0  3.0       5.0\n",
       "Phoenix                5.0   2.0   5.0  3.0     5.0     5.0  5.0       4.0\n",
       "Slightly Stoopid       1.5   3.5   1.0  4.5     NaN     4.5  4.0       2.5\n",
       "The Strokes            2.5   NaN   NaN  4.0     4.0     4.0  5.0       3.0\n",
       "Vampire Weekend        2.0   3.0   NaN  2.0     1.0     4.0  NaN       NaN"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"img_data/users.csv\",index_col='Unnamed: 0')\n",
    "\n",
    "print ('\\n')\n",
    "print  (\"OUR BAND-RATING UTILITY MATRIX\")\n",
    "print (\"\\n\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-  Let's suppose we want to recommend a band to Angelica. \n",
    "-  We will search to find a user similiar to Angelica, and based on that person's favorites we can recommend a band or two\n",
    "-  But how do we find someone 'similar'? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We discover similiar users by using distance & similiarity metrics:\n",
    "\n",
    "1)** Manhattan Distance:  **    \n",
    "\n",
    "$distance = \\sum{|x_1 - y_1|}$\n",
    "\n",
    "    \n",
    "    \n",
    "2) **Euclidean Distance:   **\n",
    "\n",
    "$distance = {\\sqrt{\\sum(x_1 - x_2)^2}}$\n",
    "\n",
    "    \n",
    "\n",
    "3)** Pearson Correlation:  **\n",
    "\n",
    "$r=\\begin{equation*}\n",
    "(\\left( \\sum_{k=1}^n (x_i- \\bar{x})(y_i- \\bar{y}) \\right)\\above 1pt\\sqrt{( \\sum_{k=1}^n (x_i- \\bar{x})^2})\\sqrt{( \\sum_{k=1}^n (y_i- \\bar{y})^2}))\n",
    "\\end{equation*}$\n",
    "\n",
    "    \n",
    "\n",
    "4) **Cosine Similiarity: **     \n",
    "\n",
    "cos(x,y)     = $x.y\\above 1pt\\ ||x||*||y||$ \n",
    "\n",
    "=   $(\\left( \\sum_{k=1}^n (x_i)(y_i) \\right)\\above 1pt\\sqrt{( \\sum_{k=1}^n (x_i})^2\\sqrt{( \\sum_{k=1}^n (y_i})^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Angelica</th>\n",
       "      <th>Bill</th>\n",
       "      <th>Chan</th>\n",
       "      <th>Dan</th>\n",
       "      <th>Hailey</th>\n",
       "      <th>Jordyn</th>\n",
       "      <th>Sam</th>\n",
       "      <th>Veronica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Blues Traveler</th>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Broken Bells</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Deadmau5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Norah Jones</th>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phoenix</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Slightly Stoopid</th>\n",
       "      <td>1.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Strokes</th>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vampire Weekend</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Angelica  Bill  Chan  Dan  Hailey  Jordyn  Sam  Veronica\n",
       "Blues Traveler         3.5   2.0   5.0  3.0     NaN     NaN  5.0       3.0\n",
       "Broken Bells           2.0   3.5   1.0  4.0     4.0     4.5  2.0       NaN\n",
       "Deadmau5               NaN   4.0   1.0  4.5     1.0     4.0  NaN       NaN\n",
       "Norah Jones            4.5   NaN   3.0  NaN     4.0     5.0  3.0       5.0\n",
       "Phoenix                5.0   2.0   5.0  3.0     5.0     5.0  5.0       4.0\n",
       "Slightly Stoopid       1.5   3.5   1.0  4.5     NaN     4.5  4.0       2.5\n",
       "The Strokes            2.5   NaN   NaN  4.0     4.0     4.0  5.0       3.0\n",
       "Vampire Weekend        2.0   3.0   NaN  2.0     1.0     4.0  NaN       NaN"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n"
     ]
    }
   ],
   "source": [
    "# cityblock~ Manhattan distance \n",
    "from scipy.spatial.distance import cityblock\n",
    "\n",
    "test=df[['Angelica','Bill']]\n",
    "test=test.dropna()\n",
    "\n",
    "#Calculating the distance between Angelica and Bill: \n",
    "dist=cityblock(test.Angelica,test.Bill)\n",
    "print (dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pearson Correlation**\n",
    "\n",
    "- As we can expect, average scores can differ greatly between different users\n",
    "- Pearson correlation will address this for us!\n",
    "\n",
    "\n",
    "$r=\\begin{equation*}\n",
    "(\\left( \\sum_{k=1}^n (x_i- \\bar{x})(y_i- \\bar{y}) \\right)\\above 1pt\\sqrt{( \\sum_{k=1}^n (x_i- \\bar{x})^2})\\sqrt{( \\sum_{k=1}^n (y_i- \\bar{y})^2}))\n",
    "\\end{equation*}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Angelica</th>\n",
       "      <th>Bill</th>\n",
       "      <th>Chan</th>\n",
       "      <th>Dan</th>\n",
       "      <th>Hailey</th>\n",
       "      <th>Jordyn</th>\n",
       "      <th>Sam</th>\n",
       "      <th>Veronica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Angelica</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.904053</td>\n",
       "      <td>0.819782</td>\n",
       "      <td>-0.357941</td>\n",
       "      <td>0.617961</td>\n",
       "      <td>0.763975</td>\n",
       "      <td>0.281867</td>\n",
       "      <td>0.829386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bill</th>\n",
       "      <td>-0.904053</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.975900</td>\n",
       "      <td>0.657376</td>\n",
       "      <td>-0.639064</td>\n",
       "      <td>-0.709299</td>\n",
       "      <td>-0.816497</td>\n",
       "      <td>-0.755929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chan</th>\n",
       "      <td>0.819782</td>\n",
       "      <td>-0.975900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.963087</td>\n",
       "      <td>0.703526</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>0.766965</td>\n",
       "      <td>0.274774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dan</th>\n",
       "      <td>-0.357941</td>\n",
       "      <td>0.657376</td>\n",
       "      <td>-0.963087</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.133631</td>\n",
       "      <td>-0.041523</td>\n",
       "      <td>-0.485913</td>\n",
       "      <td>-0.750568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hailey</th>\n",
       "      <td>0.617961</td>\n",
       "      <td>-0.639064</td>\n",
       "      <td>0.703526</td>\n",
       "      <td>0.133631</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.728296</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jordyn</th>\n",
       "      <td>0.763975</td>\n",
       "      <td>-0.709299</td>\n",
       "      <td>0.801784</td>\n",
       "      <td>-0.041523</td>\n",
       "      <td>0.728296</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.183340</td>\n",
       "      <td>0.745815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sam</th>\n",
       "      <td>0.281867</td>\n",
       "      <td>-0.816497</td>\n",
       "      <td>0.766965</td>\n",
       "      <td>-0.485913</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>-0.183340</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.559017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Veronica</th>\n",
       "      <td>0.829386</td>\n",
       "      <td>-0.755929</td>\n",
       "      <td>0.274774</td>\n",
       "      <td>-0.750568</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.745815</td>\n",
       "      <td>-0.559017</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Angelica      Bill      Chan       Dan    Hailey    Jordyn  \\\n",
       "Angelica  1.000000 -0.904053  0.819782 -0.357941  0.617961  0.763975   \n",
       "Bill     -0.904053  1.000000 -0.975900  0.657376 -0.639064 -0.709299   \n",
       "Chan      0.819782 -0.975900  1.000000 -0.963087  0.703526  0.801784   \n",
       "Dan      -0.357941  0.657376 -0.963087  1.000000  0.133631 -0.041523   \n",
       "Hailey    0.617961 -0.639064  0.703526  0.133631  1.000000  0.728296   \n",
       "Jordyn    0.763975 -0.709299  0.801784 -0.041523  0.728296  1.000000   \n",
       "Sam       0.281867 -0.816497  0.766965 -0.485913  0.555556 -0.183340   \n",
       "Veronica  0.829386 -0.755929  0.274774 -0.750568  0.000000  0.745815   \n",
       "\n",
       "               Sam  Veronica  \n",
       "Angelica  0.281867  0.829386  \n",
       "Bill     -0.816497 -0.755929  \n",
       "Chan      0.766965  0.274774  \n",
       "Dan      -0.485913 -0.750568  \n",
       "Hailey    0.555556  0.000000  \n",
       "Jordyn   -0.183340  0.745815  \n",
       "Sam       1.000000 -0.559017  \n",
       "Veronica -0.559017  1.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can panda's 'corr' function to create a pearson square matrix \n",
    "pearson=df.corr('pearson')\n",
    "        \n",
    "pearson.head(8)\n",
    "\n",
    "# Note: scipy.stats also has a 'pearson' function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating Pearson for Angelica and Bill:\n",
    "\n",
    "\n",
    "$r=\\begin{equation*}\n",
    "(\\left( \\sum_{k=1}^n (x_i- \\bar{x})(y_i- \\bar{y}) \\right)\\above 1pt\\sqrt{( \\sum_{k=1}^n (x_i- \\bar{x})^2})\\sqrt{( \\sum_{k=1}^n (y_i- \\bar{y})^2}))\n",
    "\\end{equation*}$\n",
    "\n",
    "\n",
    "<img src=\"img_data/pearson1.png\", width=1000>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfgAAAHICAYAAACiUvjSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113cb00b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing our Pearson Correlations.. \n",
    " \n",
    "plt.rcParams['figure.figsize'] = (8, 8) \n",
    "\n",
    "cor=df.corr('pearson')\n",
    "\n",
    "plt.imshow(cor,cmap='Reds',interpolation='none')\n",
    "plt.colorbar();\n",
    "plt.title(\"Band Rating Correlation \")\n",
    "plt.xticks(range(len(cor)), cor.columns)\n",
    "plt.yticks(range(len(cor)), cor.columns);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####And Lastly..\n",
    "\n",
    "**Cosine Similiarity: **     \n",
    "\n",
    "cos(x,y)     = $x.y\\above 1pt\\ ||x||$ \n",
    "\n",
    "=   $(\\left( \\sum_{k=1}^n (x_i)(y_i) \\right)\\above 1pt\\sqrt{( \\sum_{k=1}^n (x_i})^2\\sqrt{( \\sum_{k=1}^n (y_i})^2)$\n",
    "\n",
    "\n",
    "- We can see that '0' (NaN) values will not affect our calculations\n",
    "\n",
    "#### Calculating Pearson for Angelica and Bill:\n",
    "\n",
    "<img src=\"img_data/cos_sim.png\", width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Angelica</th>\n",
       "      <th>Bill</th>\n",
       "      <th>Chan</th>\n",
       "      <th>Dan</th>\n",
       "      <th>Hailey</th>\n",
       "      <th>Jordyn</th>\n",
       "      <th>Sam</th>\n",
       "      <th>Veronica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Angelica</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.521021</td>\n",
       "      <td>0.391977</td>\n",
       "      <td>0.648221</td>\n",
       "      <td>0.812076</td>\n",
       "      <td>0.717412</td>\n",
       "      <td>0.634953</td>\n",
       "      <td>0.355001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bill</th>\n",
       "      <td>0.521021</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871002</td>\n",
       "      <td>0.645456</td>\n",
       "      <td>0.813040</td>\n",
       "      <td>0.824127</td>\n",
       "      <td>0.807072</td>\n",
       "      <td>0.890178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chan</th>\n",
       "      <td>0.391977</td>\n",
       "      <td>0.871002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.359026</td>\n",
       "      <td>0.563439</td>\n",
       "      <td>0.817293</td>\n",
       "      <td>0.549195</td>\n",
       "      <td>0.884798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dan</th>\n",
       "      <td>0.648221</td>\n",
       "      <td>0.645456</td>\n",
       "      <td>0.359026</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.927341</td>\n",
       "      <td>0.628327</td>\n",
       "      <td>0.805384</td>\n",
       "      <td>0.554289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hailey</th>\n",
       "      <td>0.812076</td>\n",
       "      <td>0.813040</td>\n",
       "      <td>0.563439</td>\n",
       "      <td>0.927341</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.778867</td>\n",
       "      <td>0.870659</td>\n",
       "      <td>0.649528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Angelica      Bill      Chan       Dan    Hailey    Jordyn  \\\n",
       "Angelica  1.000000  0.521021  0.391977  0.648221  0.812076  0.717412   \n",
       "Bill      0.521021  1.000000  0.871002  0.645456  0.813040  0.824127   \n",
       "Chan      0.391977  0.871002  1.000000  0.359026  0.563439  0.817293   \n",
       "Dan       0.648221  0.645456  0.359026  1.000000  0.927341  0.628327   \n",
       "Hailey    0.812076  0.813040  0.563439  0.927341  1.000000  0.778867   \n",
       "\n",
       "               Sam  Veronica  \n",
       "Angelica  0.634953  0.355001  \n",
       "Bill      0.807072  0.890178  \n",
       "Chan      0.549195  0.884798  \n",
       "Dan       0.805384  0.554289  \n",
       "Hailey    0.870659  0.649528  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "df_fill=df.fillna(0)\n",
    "\n",
    "sims=cosine_similarity(df.fillna(0))\n",
    "\n",
    "\n",
    "sims_df=pd.DataFrame(sims,index=df_fill.columns,columns=df_fill.columns)\n",
    "sims_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which Similiarity Metric Should we use ?\n",
    "\n",
    "-  If there are large differences between user means --->  Use Pearson\n",
    "-  If data is large and dense (mostly non-zero values) --->  Use Euclidean \n",
    "-  If data is sparse (many NaNs.. ) ---> Use Cosine Similiarity\n",
    "   (if observ ~ 0, essentially dot product ignores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  K-NN \n",
    "\n",
    "Our recommender will provide results based on the closest matches.\n",
    "However, imagine what could happen if these results were only based off of only one user.\n",
    "\n",
    "What if you and I really liked the same music?  \n",
    "\n",
    "But I also really like Barbara Streisand ..\n",
    "If our recommender is only based off of one user, our system will send a recommendation of Streisand to you!\n",
    "\n",
    "In order to avoid providing 'eccentric' recommendations: \n",
    "We can use KNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"babs1.png\",width=200>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Fix -> try to change structure into dataframe.. ??\n",
    "def computeNearestNeighbor(username, df):\n",
    "    \"\"\"creates a sorted list of users based on distance to username\"\"\"\n",
    "    distances = []\n",
    "    for i in df.columns:\n",
    "        # make sure user is not self..\n",
    "        if i != username:\n",
    "            #pulling values from our pearson Matrix - \n",
    "            sim = pearson[i][username]\n",
    "            distances.append((sim, i))\n",
    "    # sort based on distance -- closest first\n",
    "    distances.sort()\n",
    "    # Just return the top 4 Neighbors\n",
    "    return distances[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.6179606196162849, 'Hailey'),\n",
       " (0.76397486054754327, 'Jordyn'),\n",
       " (0.81978229472994113, 'Chan'),\n",
       " (0.82938597622637422, 'Veronica')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeNearestNeighbor('Angelica',df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deadmau5']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Refine for definition.. \n",
    "\n",
    "def recommendation(username,df):\n",
    "    # first find nearest neighbor\n",
    "    nearest = computeNearestNeighbor(username, df)[-1][1]\n",
    "    recommendations = []\n",
    "    # now find bands neighbor rated that user didn't\n",
    "    neighborRatings = df[nearest]\n",
    "    userRatings = df[username]\n",
    "    for artist in neighborRatings.index:\n",
    "        if np.isnan(userRatings[artist]):\n",
    "            recommendations.append(artist)\n",
    "    return recommendations\n",
    "\n",
    "recommendation('Angelica',df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>a perfect circle</th>\n",
       "      <th>abba</th>\n",
       "      <th>ac/dc</th>\n",
       "      <th>adam green</th>\n",
       "      <th>aerosmith</th>\n",
       "      <th>afi</th>\n",
       "      <th>air</th>\n",
       "      <th>alanis morissette</th>\n",
       "      <th>alexisonfire</th>\n",
       "      <th>...</th>\n",
       "      <th>timbaland</th>\n",
       "      <th>tom waits</th>\n",
       "      <th>tool</th>\n",
       "      <th>tori amos</th>\n",
       "      <th>travis</th>\n",
       "      <th>trivium</th>\n",
       "      <th>u2</th>\n",
       "      <th>underoath</th>\n",
       "      <th>volbeat</th>\n",
       "      <th>yann tiersen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  286 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  a perfect circle  abba  ac/dc  adam green  aerosmith  afi  air  \\\n",
       "0     1                 0     0      0           0          0    0    0   \n",
       "1    33                 0     0      0           1          0    0    0   \n",
       "2    42                 0     0      0           0          0    0    0   \n",
       "3    51                 0     0      0           0          0    0    0   \n",
       "4    62                 0     0      0           0          0    0    0   \n",
       "\n",
       "   alanis morissette  alexisonfire      ...       timbaland  tom waits  tool  \\\n",
       "0                  0             0      ...               0          0     0   \n",
       "1                  0             0      ...               0          0     0   \n",
       "2                  0             0      ...               0          0     0   \n",
       "3                  0             0      ...               0          0     0   \n",
       "4                  0             0      ...               0          0     0   \n",
       "\n",
       "   tori amos  travis  trivium  u2  underoath  volbeat  yann tiersen  \n",
       "0          0       0        0   0          0        0             0  \n",
       "1          0       0        0   0          0        0             0  \n",
       "2          0       0        0   0          0        0             0  \n",
       "3          0       0        0   0          0        0             0  \n",
       "4          0       0        0   0          0        0             0  \n",
       "\n",
       "[5 rows x 286 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local='img_data/data.txt'\n",
    "data=pd.read_csv(local,delimiter=',')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Explicit vs Implicit Utility Matrices **\n",
    "\n",
    "Our previous matrix (for user-user) was an 'explicit' ratings matrix.  Explicit ratings reflect actual user ratings (ex: 4 stars).    \n",
    "Note that the above matrix is an implicit ratings matrix, which is one that simply reflects behavoir (ie: did 'user 33' previously listen to adam green (or not)?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12  Model Based Methods:   \n",
    "\n",
    "Types of Model Based Methods:\n",
    "\n",
    "     *  Matrix Factorization \n",
    "     *  Probabilistic models (Clustering models, Bayesian networks) \n",
    "         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 1.12 a) Matrix Factorization:\n",
    " \n",
    "###### SGD-SVD (Stochastic Gradient Descent Singular Value Decomposition):\n",
    "\n",
    " \n",
    "Where user-based and item-based algorithms can be easily implemented and intuitive, Matrix Factorization methods can be more effective because they allow us to discover latent features underlying the interactions between users and items (think PCA !).\n",
    "\n",
    "The Matrix Factorization Process:\n",
    "- Allows us to factorize two or more matrices such that when you multiply them back together you will get (an approximation for) the original matrix.\n",
    "\n",
    "$$ {P}\\cdot{Q}^T = \\hat{R} $$ \n",
    "\n",
    "1) Let's imagine we have the following Utility Matrix: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R = [[5,3,0,1],\n",
    "     [4,0,0,1],\n",
    "     [1,1,0,5],\n",
    "     [1,0,0,4],\n",
    "     [0,1,5,4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Our task is predict the missing ratings (We can think of this as imputing methods used to fill NaN's)\n",
    "\n",
    "3) Consider our Utility Matrix to be of size |U| X |D| ( Where: U ~ Users, D ~ Items )\n",
    "\n",
    "4) For this example, we will be returning (2) matrices: P & Q. We can assume that the our matrix P will contain the 'strength' of the Users and matrix Q will contain the 'strength' of the Items.  \n",
    "\n",
    "5)How do we obtain P & Q ? \n",
    "\n",
    "- Initialize two matrices with some values ( we will use Gaussian dist.)\n",
    "\n",
    "- Calculate how different their product is from R - > Using Gradient Descent (SGD), we minimize these differences (~ error)  iteratively (Our goal is to find the local minimum of the difference).\n",
    "\n",
    "\n",
    "$ (e_{ij})^2 = (r_{ij}-\\hat{r_{ij}})^2 = (r_{ij}-\\sum_{k=1}^K {p_{ij}}q_{kj})^2 $\n",
    " \n",
    "\n",
    "6) In order to minimize our error, we have to know which direction in which to modify  our p_ik and q_kj values.  In other words, we have to find the gradient at our current values.  As we know from calculas class, in order to minimize we differentiate with respect to either variable.\n",
    "\n",
    "\n",
    "\n",
    "Alpha in the equation above is a 'step' constant which will determine the rate at which we will be approaching the minimum.  Ideally, we will work with small values (ex : \n",
    "alpha = 0.0002)\n",
    "\n",
    "Using the above update rules iteratively, we update our P  & Q matrix values until the error converges to a minimum.\n",
    "Once the errror converges to a min., we can stop the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Our Final Step : Regularization\n",
    "\n",
    "A common extension this above 'basic' algorithm is to introduce regularization to avoid overfitting (B~ .02): \n",
    "\n",
    "<img src='img_data/beta.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters : U, V and features: D \n",
    "# this will establish matrix factorization using Gradient Descent\n",
    "# given matrix is minimized\n",
    "\n",
    "def matrix_factorization(R, P, Q, K, steps=5000, alpha=0.0002, beta=0.02):\n",
    "\n",
    "    Q = Q.T\n",
    "    error =[]\n",
    "    for step in range(steps):\n",
    "        for i in range(len(R)):\n",
    "            for j in range(len(R[i])):\n",
    "                \n",
    "                # if rated:\n",
    "                if R[i][j] > 0:\n",
    "                    \n",
    "                    # calc. error \n",
    "                    eij = R[i][j] - numpy.dot(P[i,:],Q[:,j])\n",
    "                    \n",
    "                    # Here alpha is a constant whose value determines the rate of the approaching min.. \n",
    "                    \n",
    "                    # our parameter Beta is used to control the magnitudes of the user-feature and item-features vectors.\n",
    "                    # such that P and Q would give a good approximation of R\n",
    "                    for k in range(K):\n",
    "                        # update P..\n",
    "                        P[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])\n",
    "                        \n",
    "                        # update Q.. \n",
    "                        Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j])\n",
    "\n",
    "        e = 0\n",
    "\n",
    "        for i in range(len(R)):\n",
    "\n",
    "            for j in range(len(R[i])):\n",
    "                \n",
    "                # if rating is greater than 0, calculate error\n",
    "                if R[i][j] > 0:\n",
    "\n",
    "                    e += pow(R[i][j] - numpy.dot(P[i,:],Q[:,j]), 2)\n",
    "                    \n",
    "                        \n",
    "        error.append(e)              \n",
    "        if e < 0.001:\n",
    "\n",
    "            break\n",
    "\n",
    "    return P, Q.T,error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R = [[5,3,0,1],\n",
    "     [4,0,0,1],\n",
    "     [1,1,0,5],\n",
    "     [1,0,0,4],\n",
    "     [0,1,5,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R=np.array(R)\n",
    "\n",
    "N= len(R)\n",
    "M=len(R[0])\n",
    "\n",
    "# We can assign \n",
    "K=2\n",
    "\n",
    "# find p,q:   init U and M with randomized value between 0.0 and 1.0 with standard Gaussian distribution\n",
    "P=np.random.rand(N,K)\n",
    "Q=np.random.rand(M,K)\n",
    "            \n",
    "nP,nQ,error= matrix_factorization(R,P,Q,K)\n",
    "nR=np.dot(nP,nQ.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P Matrix\n",
      "[[ 2.34587756 -0.13158779]\n",
      " [ 1.83780288 -0.01400509]\n",
      " [ 0.65294017  1.88146746]\n",
      " [ 0.54394162  1.50222694]\n",
      " [ 1.27260674  1.35640386]]\n",
      "(5, 2)\n",
      "\n",
      "\n",
      "Q Matrix\n",
      "[[ 2.1394866  -0.14700352]\n",
      " [ 1.19652565 -0.05602743]\n",
      " [ 2.43299525  1.29259296]\n",
      " [ 0.56232064  2.44416954]]\n",
      "[[ 2.1394866  -0.14700352]\n",
      " [ 1.19652565 -0.05602743]\n",
      " [ 2.43299525  1.29259296]\n",
      " [ 0.56232064  2.44416954]]\n",
      "(4, 2)\n"
     ]
    }
   ],
   "source": [
    "print (\"P Matrix\")\n",
    "print (nP)\n",
    "print (nP.shape)\n",
    "print ('\\n')\n",
    "\n",
    "print (\"Q Matrix\")\n",
    "print (nQ)\n",
    "print (nQ)\n",
    "print (nQ.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Matrix\n",
      "[[5 3 0 1]\n",
      " [4 0 0 1]\n",
      " [1 1 0 5]\n",
      " [1 0 0 4]\n",
      " [0 1 5 4]]\n",
      "\n",
      "\n",
      "R_hat Matrix\n",
      "[[ 5.03831747  2.81427519  5.53741951  0.99751251]\n",
      " [ 3.93401344  2.19976295  4.4532628   0.99920368]\n",
      " [ 1.1203744   0.67584588  4.02057192  4.96578719]\n",
      " [ 0.94292315  0.56667418  3.26517533  3.97756693]\n",
      " [ 2.52332892  1.44671078  4.84952423  4.03089404]]\n"
     ]
    }
   ],
   "source": [
    "print (\"R Matrix\")\n",
    "print (R)\n",
    "\n",
    "\n",
    "print ('\\n')\n",
    "print (\"R_hat Matrix\")\n",
    "print (nR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "plt.plot(error)\n",
    "plt.title (\"Diminishing Error\")\n",
    "\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "**Further Reading and References: **\n",
    "\n",
    "        \n",
    "- Netflix Prize:  http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/\n",
    "        \n",
    "- See Crab module: Recommender Framework in Python\n",
    "    http://muricoca.github.io/crab/\n",
    "        \n",
    "- 'Mining Massive Datasets', Jure Leskovec (Stanford Univ)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
