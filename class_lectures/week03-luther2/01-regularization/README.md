### Schedule

**9:00 am**: [Pair Problem](pair.md)

		Audrey, Arina
		Adam, Christine.L
		Jit, Pavan
		Jeremy, Brad.S
		Brian, Wendy
		Jon, Malik
		Brad.D, Sungwan
		Summer, Jaydon
		Louisa, Rishabh
		Will, Chris
		Kevin, Browning
		Matias, Mauro
		Christine.C, Laura

**10:00 am**: [Null Hypothesis](null_hypothesis_testing.md)

**10:15 am**: [Regularization](regularization.pdf)

**10:45 am**: [Regularization via Jupyter](Regularization.ipynb)

**12:00 am**: Lunch

**1:00 pm**: Investigation Presentation

**1:15 pm**: Work on all things!


### Further "Reading"

 * [Regularized linear regression with scikit-learn](http://www.datarobot.com/blog/regularized-linear-regression-with-scikit-learn/): This goes over some of the theory we discussed, and shows using regularization on an actual scikit learn example. It uses Lasso instead of Ridge. The only difference between Lasso and Ridge regularization is this: Ridge adds the sum of beta squares to the cost, while Lasso adds sum of beta absolute values. Other than that functional form, the idea is pretty much the same. The interface of using LinearRegression() or Ridge(alpha) or Lasso(alpha) is also exactly the same.
 * [Ten minute video lecture on regularization](https://www.youtube.com/watch?v=fx-TqOzjDbM): Another ten minute lecture by Andrew Ng on how the cost function manipulation works in regularization. It helps build intuition.
 * [Regularization Viz](http://jonhanke.github.io/Regularization-for-Regular-People/)
 
