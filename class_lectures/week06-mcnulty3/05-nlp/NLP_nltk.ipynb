{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with NLTK (.. and a little sklearn) \n",
    "\n",
    "Natural Language Processing with the Natural Language Toolkit\n",
    "\n",
    "[nltk](http://www.nltk.org/) is a Python package for NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pip install nltk\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of NLTK depends on additional data which you'll have to download. Use `nltk.download()` to get at least the following:\n",
    "\n",
    " * averaged_perceptron_tagger (in models)\n",
    " * maxent_treebank_pos_tagger (in models)\n",
    " * punkt (in models)\n",
    " * maxent_ne_chunk (in models)\n",
    " * words (in corpora)\n",
    "\n",
    "You can install these and continue without restarting your kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"\"\"Hello. How are you, dear Mr. Sir? Are you well?\n",
    "          Here: drink this! It will make you feel better.\n",
    "          I mean, it won't make you feel worse!\"\"\"\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TreebankWordTokenizer assumes that our input has already been segmented into sentences..\n",
    "\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(sentences[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "words = word_tokenize(sentences[5])\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "wordpunct_tokenize(sentences[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo of different tokenizers: http://text-processing.com/demo/tokenize/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "words=pos_tag(word_tokenize(\"Who's going to that thing today?\"))\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some of POS tags: \n",
    "WP: wh-pronoun (\"who\", \"what\")  \n",
    "VBZ: verb, 3rd person sing. present (\"takes\")  \n",
    "VBG: verb, gerund/present participle (\"taking\")  \n",
    "TO: to (\"to go\", \"to him\")   \n",
    "DT: determiner (\"the\", \"this\")  \n",
    "NN: noun, singular or mass (\"door\")  \n",
    ".: Punctuation (\".\", \"?\")  \n",
    "\n",
    "All tags: http://www.monlp.com/2011/11/08/part-of-speech-tags/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "Extracting phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## the 'named entity' chunker!  ne_chunk utilizes \n",
    "\n",
    "from nltk.chunk import ne_chunk\n",
    "words = word_tokenize(\"\"\"I'm Julia and I'm here to say\n",
    "                         I love NLTK in a major way.\"\"\")\n",
    "tags = pos_tag(words)\n",
    "tree = ne_chunk(tags)\n",
    "print(tags)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words2 = word_tokenize(\"First National Bank announced earnings!\")\n",
    "tags2=pos_tag(words2)\n",
    "tree2=ne_chunk(tags2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree2.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Included text corpora\n",
    "\n",
    "Also install these!\n",
    "\n",
    " * movie_reviews: Imdb reviews characterized as pos & neg  \n",
    " * treebank: tagged and parsed Wall Street Journal text  \n",
    " * brown: tagged & categorized English text (news, fiction, etc)  \n",
    "\n",
    "(There are over 60 others.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A chunked corpora reader..\n",
    "\n",
    "from nltk.corpus import treebank_chunk\n",
    "treebank_chunk.tagged_sents()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "treebank_chunk.chunked_sents()[0].draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tree=treebank_chunk.chunked_sents()[0]\n",
    "' '.join([w for w, t in tree.leaves()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pip install textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "GATSBY_TEXT = \"\"\"In my younger and more vulnerable years my father\n",
    "                 gave me some advice that I've been turning over\n",
    "                 in my mind ever since. \"Whenever you feel like\n",
    "                 criticizing any one,\" he told me, \"blah blah blah.\"\"\"\n",
    "\n",
    "gatsby = TextBlob(GATSBY_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gatsby.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gatsby.noun_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  How do you really feel?    TextBlob:  Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TextBlob(\"Oh my god I love this bootcamp, it's so awesome.\").sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TextBlob(\"it's so awesome\").sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TextBlob(\"Oh my god.\").sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TextBlob(\"I love this bootcamp.\").sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TextBlob(\"Oh my god I love this bootcamp.\").sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TextBlob(\"it's so awesome.\").sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(TextBlob(\"I hate cupcakes.\").sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gatsby.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gatsby.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gatsby.sentences[0].words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "for word in TextBlob(\"Are you running in two marathons?\").words:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see different nltk stemmers in effect:\n",
    "http://text-processing.com/demo/stem/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word, count in gatsby.word_counts.items():\n",
    "    print(\"%15s %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_count(item):\n",
    "    return item[1]\n",
    "\n",
    "for word, count in sorted(gatsby.word_counts.items(), key=get_count, reverse=True):\n",
    "    print(\"%15s %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Reviews \n",
    "(without stopwords!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "fileids = movie_reviews.fileids()[:100]\n",
    "doc_words = [movie_reviews.words(fileid) for fileid in fileids]\n",
    "documents = [' '.join(words) for words in doc_words]\n",
    "print(documents[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top bigrams in reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop += ['.', ',', '(', ')', \"'\", '\"']\n",
    "stop = set(stop)\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "n = 2\n",
    "for doc in documents:\n",
    "    words = TextBlob(doc).words\n",
    "    words = [w for w in words if w not in stop]\n",
    "    bigrams = ngrams(words, n)\n",
    "    counter += Counter(bigrams)\n",
    "\n",
    "for phrase, count in counter.most_common(30):\n",
    "    print('%20s %i' % (\" \".join(phrase), count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Sklearn algorithms with text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "CountVectorizer:  Convert a collection of text documents to a matrix of token counts\n",
    "This implementation produces a sparse representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = ['That is should come to this!', 'This above all: to thine own self be true.', 'Something is rotten in the state of Denmark.']\n",
    "\n",
    "# CountVectorizer is a class; so `vectorizer` below represents an instance of that object.\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "# call `fit` to build the vocabulary\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# then, use `get_feature_names` to return the tokens\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "# finally, call `transform` to convert text to a bag of words\n",
    "x = vectorizer.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Sparse Matrix')\n",
    "# A compressed version; the \"sparse\" matrix.\n",
    "print(type(x))\n",
    "print(x)\n",
    "\n",
    "print ('Matrix')\n",
    "x_back = x.toarray()\n",
    "print(type(x_back))\n",
    "print(x_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(x_back, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### TF: frequency in this document\n",
    "#### IDF: inverse frequency in the corpus\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2))\n",
    "doc_vectors = vectorizer.fit_transform(documents)\n",
    "\n",
    "classes = np.array(['pos']*50 + ['neg']*50)\n",
    "\n",
    "\n",
    "model = MultinomialNB().fit(doc_vectors, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(GATSBY_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gatsby_vector = vectorizer.transform([GATSBY_TEXT])\n",
    "model.predict(gatsby_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
